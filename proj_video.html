<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Rapid Multi-modal Video Analytics</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">

        <header>
            <h1>Hongyuan Zhu</h1>
            <p>
            <small>hongyuanzhu.cn (AT) gmail (DOT) com </small><br><br>
            <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hongyuan" target="_blank">[DBLP]</a>  <br>
            <a href="https://scholar.google.com/citations?user=XTk3sYAAAAAJ&hl=en" target="_blank">[Google Scholar]</a> </p> <br>
            <p class="view"><a href="index.html">Homepage</a></p>
            <p class="view"><a href="sub_publication.html">Publications</a><br>
            <!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
            </header>

      <section>

<h2>
<a id="project_title" class="anchor" href="#project_title" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rapid Multi-modal Video Analytics</h2>




<h4>
<a id="Introduction-page" class="anchor" href="#Introduction-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction:</h4>

<p>
    Over the last decade, with the proliferation of surveillance cameras and mobile devices, 
    there has been an exponential growth in the number of videos being captured and archived.  
    Develop holistic models which can recognize scenes, detecting objects and understanding human actions 
    are of great importance due to various applications. For example, in video surveillance, it is important 
    to detect suspicious events, objects, and people. For healthcare, detecting a falling down activity of a 
    senior people at home can enable timely rescue. While most current models approach the problem using full 
    supervision which requires extensive annotation and hand-craft heuristics, we approach the problems 
    using weak supervision from a cognitive view in a principled optimization framework.
</p>

<div style="text-align: center; display: block; margin-right: auto;">
<img src="sub_img/proj_video.JPG" border="0" width="1000"><br></div><br>

<hr />
<h4>Paper:</h4>
    
<ul>

<li>Hongyuan Zhu, Romain Vial,  Shijian Lu  
        <strong>"TORNADO: A Spatio-Temporal Convolutional Regression Network for Video
                Action Proposal"</strong>, 
        <em>IEEE International Conference on Computer Vision (ICCV2017)</em>
        <a href="http://www.ntu.edu.sg/home/shijian.lu/Publications/Tornado%20-%20A%20spatio-temporal%20convolutional%20regression%20network%20for%20video%20action%20proposal.pdf" target="_blank">[PDF]</a> 
        <a href="***" target="_blank"><font color="#ff0000">[Code]</font></a>
</li>

<li> Hongyuan Zhu*, Romain Vial*, Shijian Lu, Xi Peng, Huazhu Fu, Yonghong Tian, Xianbin Cao, 
    <strong>"YoTube: Searching Action Proposal via Recurrent and Static Regression Networks."</strong>, 
    <em>IEEE Transactions on Image Processing (TIP), A*STAR Research Highlight</em> , 2018. 
<a href="https://arxiv.org/pdf/1706.08218.pdf" target="_blank">[PDF]</a> </li>

<li>Romain Vial, Hongyuan Zhu, Yonghong Tian, Shijian Lu
        <strong>"Search Video Action Proposal with Static and Recurrent YOLO"</strong>, 
        <em>IEEE International Conference on Image Processing (ICIP2017, Student Travel Grant, Oral)</em>
        <a href="https://ieeexplore.ieee.org/document/8296639" target="_blank">[PDF]</a> 
        <a href="***" target="_blank"><font color="#ff0000">[Code]</font></a>
</li>
</ul>

<hr />
<h4>Media Coverage:</h4>
<ul>
    <li> <a href="https://research.a-star.edu.sg/research/7911/image-analysis-tool-helps-pick-out-human-actions" target="_blank">"Image Analysis Tool Helps Pick Out Human Action"</a>ï¼Œ A*STAR Research Highlights (Top 50 significant works among 25 A*STAR research institutes are selected for publication.)</li> 
</ul>

<hr />
<h4>Related Works:</h4>

<ul>
  <li>Xi Peng, Joey Tianyi Zhou, Hongyuan Zhu, <strong>"k-meansNet: When k-means Meets Differentiable Programming"</strong>,  
    in <em>arXiv: 1808.07292</em>, 2018.  <a href="https://arxiv.org/pdf/1808.07292" target="_blank">[PDF]</a>  </li>
    <li>Anran Wang, Anh Tuan Luu, Chuan-Sheng Foo, <ud2>Hongyuan Zhu</ud2>, Yi Tay, Vijay Chandrasekhar, <strong>"Holistic Multi-modal Memory Network for Movie Question Answering" (Top1,MovieQA2018)</strong>,  
      in <em>arXiv: 1808.07292</em>, 2018.  <a href="https://arxiv.org/pdf/1808.07292" target="_blank">[PDF]</a>  </li>

    <li> Joey Tianyi Zhou, Jiawei Du,  Hongyuan Zhu*, Xi Peng,  Yong Liu, Rick Siow Mong Goh. <strong>"AnomalyNet: An Anomaly Detection Network for Video Surveillance"</strong>, 
      <em>IEEE Transactions on Information Forensics & Security (TIFS) </em>, 2019. 
      <a href="***" target="_blank">[PDF]</a> 
      <a href="https://github.com/joeyzhouty/TIFS" target="_blank"><font color="#ff0000">[Code]</font></a>
  </li>	


</ul>

      </section>

    </div>
    <script src="../../javascripts/scale.fix.js"></script>
  </body>
</html>
